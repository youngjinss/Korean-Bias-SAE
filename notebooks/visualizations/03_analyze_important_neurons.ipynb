{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Important Neuron Analysis\n",
    "\n",
    "This notebook provides deeper analysis of \"important\" neurons (SAE features) identified through IG² scores.\n",
    "\n",
    "## Analysis Scope:\n",
    "- **Cross-Layer Trajectory**: Track how neuron importance metrics evolve from Q1 → Q2 → Q3\n",
    "- **Cross-Demographic Comparison**: Compare which neurons are important across different demographics within each layer\n",
    "\n",
    "## Filtering Method:\n",
    "- **Percentile-based**: Features above 95th and 99th percentile of IG² scores\n",
    "\n",
    "## Key Metrics:\n",
    "- **Concentration Index**: `sum(top-k scores) / sum(all scores)` - measures bias concentration\n",
    "- **Jaccard Similarity**: `|A∩B| / |A∪B|` - measures feature overlap between demographics\n",
    "- **Layer Change Ratio**: `(score_L2 - score_L1) / score_L1` - tracks importance changes\n",
    "\n",
    "## Prerequisites:\n",
    "- Run IG² computation for all 9 demographics and 3 layers\n",
    "- Results stored in `results/{stage}/{demographic}/ig2/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Add project root to path\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.visualization import ensure_korean_font\n",
    "from src.utils import load_json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Korean font\n",
    "font_name = ensure_korean_font()\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ASSETS_DIR = PROJECT_ROOT / \"notebooks\" / \"visualizations\" / \"assets\"\n",
    "ASSETS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stage\n",
    "STAGE = \"full\"\n",
    "\n",
    "# Percentile thresholds for filtering\n",
    "PERCENTILES = [95, 99]\n",
    "\n",
    "# Layer quantiles\n",
    "LAYER_QUANTILES = [\"q1\", \"q2\", \"q3\"]\n",
    "LAYER_LABELS = {\n",
    "    \"q1\": \"Layer Q1 (25%)\",\n",
    "    \"q2\": \"Layer Q2 (50%)\",\n",
    "    \"q3\": \"Layer Q3 (75%)\"\n",
    "}\n",
    "\n",
    "# Load demographics\n",
    "demo_dict = load_json(DATA_DIR / \"demographic_dict_ko.json\")\n",
    "DEMOGRAPHICS = list(demo_dict.keys())\n",
    "DEMOGRAPHIC_EN = {d: demo_dict[d]['dimension_en'] for d in DEMOGRAPHICS}\n",
    "\n",
    "print(f\"Stage: {STAGE}\")\n",
    "print(f\"Percentile Thresholds: {PERCENTILES}\")\n",
    "print(f\"Layer Quantiles: {LAYER_QUANTILES}\")\n",
    "print(f\"\\nDemographics ({len(DEMOGRAPHICS)}):\")\n",
    "for d in DEMOGRAPHICS:\n",
    "    print(f\"  - {d} ({DEMOGRAPHIC_EN[d]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ig2_results(results_dir, stage, demographic, layer_quantile):\n",
    "    \"\"\"\n",
    "    Load IG² attribution results.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'scores', 'bias_features', 'threshold', 'metadata'\n",
    "        or None if not found\n",
    "    \"\"\"\n",
    "    ig2_path = results_dir / stage / demographic / 'ig2' / f'{layer_quantile}_ig2_results.pt'\n",
    "    \n",
    "    if not ig2_path.exists():\n",
    "        # Fallback to legacy path\n",
    "        legacy_path = results_dir / stage / demographic / 'ig2' / 'ig2_results.pt'\n",
    "        if legacy_path.exists():\n",
    "            ig2_path = legacy_path\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    data = torch.load(ig2_path, map_location='cpu')\n",
    "    \n",
    "    # Extract scores\n",
    "    if isinstance(data, dict):\n",
    "        scores = data.get('feature_scores', data.get('ig2_scores', None))\n",
    "        if scores is not None and isinstance(scores, torch.Tensor):\n",
    "            scores = scores.cpu().numpy()\n",
    "        return {\n",
    "            'scores': scores,\n",
    "            'bias_features': data.get('bias_features', None),\n",
    "            'threshold': data.get('threshold', None),\n",
    "            'metadata': data.get('metadata', {})\n",
    "        }\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return {\n",
    "            'scores': data.cpu().numpy(),\n",
    "            'bias_features': None,\n",
    "            'threshold': None,\n",
    "            'metadata': {}\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Data loading function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all IG² results\n",
    "ig2_data = {}  # {demographic: {layer: {'scores': array, 'metadata': dict}}}\n",
    "\n",
    "print(\"Loading IG² results...\\n\")\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    ig2_data[demo] = {}\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        data = load_ig2_results(RESULTS_DIR, STAGE, demo, lq)\n",
    "        if data and data['scores'] is not None:\n",
    "            ig2_data[demo][lq] = data\n",
    "\n",
    "# Summary\n",
    "print(\"Loaded IG² Results Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Demographic':<20} | {'Q1':<10} | {'Q2':<10} | {'Q3':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    demo_en = DEMOGRAPHIC_EN[demo]\n",
    "    row = f\"{demo_en:<20} |\"\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        if lq in ig2_data[demo]:\n",
    "            n_features = len(ig2_data[demo][lq]['scores'])\n",
    "            row += f\" {n_features:,}    |\"\n",
    "        else:\n",
    "            row += f\" --        |\"\n",
    "    print(row)\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Percentile-Based Feature Filtering\n",
    "\n",
    "### How It's Computed:\n",
    "For each demographic-layer combination, we compute the percentile threshold from non-zero IG² scores. Features with scores >= threshold are considered \"important\". We use 95th and 99th percentiles to identify moderately and highly important features respectively.\n",
    "\n",
    "### Analysis:\n",
    "The 99th percentile captures the most critical bias-encoding features (top 1%), while 95th percentile captures a broader set (top 5%). Comparing feature counts across layers reveals where bias information is more concentrated vs. distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features_by_percentile(scores, percentile):\n",
    "    \"\"\"\n",
    "    Filter features by percentile threshold on non-zero scores.\n",
    "    \n",
    "    Args:\n",
    "        scores: numpy array of IG² scores\n",
    "        percentile: percentile threshold (e.g., 95, 99)\n",
    "    \n",
    "    Returns:\n",
    "        important_indices: indices of features above threshold\n",
    "        threshold: computed threshold value\n",
    "    \"\"\"\n",
    "    nonzero_scores = scores[scores > 0]\n",
    "    if len(nonzero_scores) == 0:\n",
    "        return np.array([]), 0.0\n",
    "    \n",
    "    threshold = np.percentile(nonzero_scores, percentile)\n",
    "    important_indices = np.where(scores >= threshold)[0]\n",
    "    return important_indices, threshold\n",
    "\n",
    "\n",
    "def compute_concentration_index(scores, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute concentration index: sum(top-k scores) / sum(all scores).\n",
    "    Higher values mean bias is concentrated in fewer features.\n",
    "    \"\"\"\n",
    "    total_score = scores.sum()\n",
    "    if total_score == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    top_k_scores = np.sort(scores)[-top_k:]\n",
    "    return top_k_scores.sum() / total_score\n",
    "\n",
    "\n",
    "print(\"Filtering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for all demographic-layer combinations\n",
    "stats_data = []\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    demo_en = DEMOGRAPHIC_EN[demo]\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        if lq not in ig2_data[demo]:\n",
    "            continue\n",
    "        \n",
    "        scores = ig2_data[demo][lq]['scores']\n",
    "        \n",
    "        row = {\n",
    "            'Demographic': demo,\n",
    "            'Demographic_EN': demo_en,\n",
    "            'Layer': lq,\n",
    "            'Layer_Label': LAYER_LABELS[lq],\n",
    "            'Max_Score': scores.max(),\n",
    "            'Max_Neuron_Idx': int(np.argmax(scores)),\n",
    "            'Total_Nonzero': int((scores > 0).sum()),\n",
    "            'Total_Score_Sum': scores.sum(),\n",
    "            'Concentration_Top10': compute_concentration_index(scores, 10),\n",
    "            'Concentration_Top50': compute_concentration_index(scores, 50),\n",
    "            'Concentration_Top100': compute_concentration_index(scores, 100),\n",
    "        }\n",
    "        \n",
    "        # Compute percentile-based filtering\n",
    "        for p in PERCENTILES:\n",
    "            indices, threshold = filter_features_by_percentile(scores, p)\n",
    "            row[f'P{p}_Count'] = len(indices)\n",
    "            row[f'P{p}_Threshold'] = threshold\n",
    "            row[f'P{p}_Indices'] = indices  # Store for later use\n",
    "        \n",
    "        stats_data.append(row)\n",
    "\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "\n",
    "print(f\"Computed statistics for {len(df_stats)} demographic-layer combinations\")\n",
    "print(\"\\nSample data:\")\n",
    "display_cols = ['Demographic_EN', 'Layer', 'Max_Score', 'Total_Nonzero', \n",
    "                'P95_Count', 'P99_Count', 'Concentration_Top10']\n",
    "print(df_stats[display_cols].head(12).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Feature count by percentile threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "demo_en_ordered = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "\n",
    "for idx, p in enumerate(PERCENTILES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Pivot\n",
    "    pivot = df_stats.pivot(index='Demographic_EN', columns='Layer', values=f'P{p}_Count')\n",
    "    pivot = pivot.reindex(demo_en_ordered)\n",
    "    pivot = pivot[['q1', 'q2', 'q3']]\n",
    "    pivot.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='YlOrRd',\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': f'Feature Count'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 11}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Important Features Count ({p}th Percentile)\\nTop {100-p}% of Non-Zero IG² Scores', \n",
    "                 fontsize=13, pad=15)\n",
    "    ax.set_xlabel('Layer', fontsize=12)\n",
    "    ax.set_ylabel('Demographic', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_percentile_counts_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nFeature Count Statistics by Percentile:\")\n",
    "print(\"=\" * 60)\n",
    "for p in PERCENTILES:\n",
    "    print(f\"\\n{p}th Percentile (Top {100-p}%):\")\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        lq_data = df_stats[df_stats['Layer'] == lq]\n",
    "        print(f\"  {LAYER_LABELS[lq]}: Mean={lq_data[f'P{p}_Count'].mean():.1f}, \"\n",
    "              f\"Min={lq_data[f'P{p}_Count'].min()}, Max={lq_data[f'P{p}_Count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Score distribution with percentile lines for one example\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "# Select 3 representative demographics\n",
    "demo_examples = DEMOGRAPHICS[:3]  # gender, ethnicity, religion\n",
    "\n",
    "for i, demo in enumerate(demo_examples):\n",
    "    demo_en = DEMOGRAPHIC_EN[demo]\n",
    "    \n",
    "    for j, lq in enumerate(LAYER_QUANTILES):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if lq in ig2_data[demo]:\n",
    "            scores = ig2_data[demo][lq]['scores']\n",
    "            nonzero = scores[scores > 0]\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax.hist(nonzero, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            # Add percentile lines\n",
    "            colors = ['orange', 'red']\n",
    "            for k, p in enumerate(PERCENTILES):\n",
    "                threshold = np.percentile(nonzero, p)\n",
    "                ax.axvline(threshold, color=colors[k], linestyle='--', linewidth=2, \n",
    "                          label=f'{p}th: {threshold:.4f}')\n",
    "            \n",
    "            ax.set_yscale('log')\n",
    "            ax.legend(loc='upper right', fontsize=9)\n",
    "        \n",
    "        ax.set_title(f'{demo_en} - {LAYER_LABELS[lq]}', fontsize=11)\n",
    "        ax.set_xlabel('IG² Score', fontsize=10)\n",
    "        ax.set_ylabel('Frequency', fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('IG² Score Distribution with Percentile Thresholds', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_score_distribution_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Cross-Layer Trajectory Analysis\n",
    "\n",
    "### How It's Computed:\n",
    "We track four metrics across layers Q1→Q2→Q3: (1) Max IG² score - peak feature importance, (2) Important feature count at 99th percentile, (3) Concentration index - top-10 scores as fraction of total, (4) Layer change ratio between consecutive layers.\n",
    "\n",
    "### Analysis:\n",
    "Increasing max scores across layers suggests bias encoding strengthens in deeper layers. Increasing feature counts indicate bias information becomes more distributed. Decreasing concentration index means bias shifts from concentrated (few features) to diffuse (many features) encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-layer trajectory: Line plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Max_Score', 'P99_Count', 'Concentration_Top10', 'Total_Nonzero']\n",
    "titles = ['Max IG² Score', 'Important Features (99th Pctl)', \n",
    "          'Concentration Index (Top-10)', 'Total Non-Zero Features']\n",
    "ylabels = ['Score', 'Count', 'Concentration', 'Count']\n",
    "\n",
    "layer_x = [1, 2, 3]  # Q1, Q2, Q3\n",
    "\n",
    "# Color palette for demographics\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(DEMOGRAPHICS)))\n",
    "\n",
    "for idx, (metric, title, ylabel) in enumerate(zip(metrics, titles, ylabels)):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    for i, demo in enumerate(DEMOGRAPHICS):\n",
    "        demo_en = DEMOGRAPHIC_EN[demo]\n",
    "        \n",
    "        values = []\n",
    "        for lq in LAYER_QUANTILES:\n",
    "            row = df_stats[(df_stats['Demographic'] == demo) & (df_stats['Layer'] == lq)]\n",
    "            if len(row) > 0:\n",
    "                values.append(row[metric].values[0])\n",
    "            else:\n",
    "                values.append(np.nan)\n",
    "        \n",
    "        ax.plot(layer_x, values, 'o-', label=demo_en, color=colors[i], \n",
    "               linewidth=2, markersize=8, alpha=0.8)\n",
    "    \n",
    "    ax.set_xticks(layer_x)\n",
    "    ax.set_xticklabels(['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)'])\n",
    "    ax.set_xlabel('Layer', fontsize=11)\n",
    "    ax.set_ylabel(ylabel, fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, pad=10)\n",
    "    ax.legend(loc='upper right', fontsize=8, ncol=2)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Cross-Layer Trajectory: Neuron Importance Metrics', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_layer_trajectory_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute layer change ratios\n",
    "change_data = []\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    demo_en = DEMOGRAPHIC_EN[demo]\n",
    "    \n",
    "    # Get values for each layer\n",
    "    values = {}\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        row = df_stats[(df_stats['Demographic'] == demo) & (df_stats['Layer'] == lq)]\n",
    "        if len(row) > 0:\n",
    "            values[lq] = {\n",
    "                'Max_Score': row['Max_Score'].values[0],\n",
    "                'P99_Count': row['P99_Count'].values[0],\n",
    "                'Concentration': row['Concentration_Top10'].values[0]\n",
    "            }\n",
    "    \n",
    "    if len(values) == 3:  # All layers available\n",
    "        for metric in ['Max_Score', 'P99_Count', 'Concentration']:\n",
    "            # Q1->Q2 change\n",
    "            if values['q1'][metric] > 0:\n",
    "                change_q1_q2 = (values['q2'][metric] - values['q1'][metric]) / values['q1'][metric]\n",
    "            else:\n",
    "                change_q1_q2 = 0\n",
    "            \n",
    "            # Q2->Q3 change\n",
    "            if values['q2'][metric] > 0:\n",
    "                change_q2_q3 = (values['q3'][metric] - values['q2'][metric]) / values['q2'][metric]\n",
    "            else:\n",
    "                change_q2_q3 = 0\n",
    "            \n",
    "            # Q1->Q3 total change\n",
    "            if values['q1'][metric] > 0:\n",
    "                change_q1_q3 = (values['q3'][metric] - values['q1'][metric]) / values['q1'][metric]\n",
    "            else:\n",
    "                change_q1_q3 = 0\n",
    "            \n",
    "            change_data.append({\n",
    "                'Demographic': demo,\n",
    "                'Demographic_EN': demo_en,\n",
    "                'Metric': metric,\n",
    "                'Q1_Q2_Change': change_q1_q2 * 100,  # as percentage\n",
    "                'Q2_Q3_Change': change_q2_q3 * 100,\n",
    "                'Q1_Q3_Change': change_q1_q3 * 100\n",
    "            })\n",
    "\n",
    "df_changes = pd.DataFrame(change_data)\n",
    "print(\"Layer change ratios computed (as %)\")\n",
    "print(df_changes.head(12).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of layer changes for Max_Score\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metric_labels = {\n",
    "    'Max_Score': 'Max IG² Score Change (%)',\n",
    "    'P99_Count': 'Feature Count Change (%)',\n",
    "    'Concentration': 'Concentration Change (%)'\n",
    "}\n",
    "\n",
    "demo_en_ordered = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "\n",
    "for idx, metric in enumerate(['Max_Score', 'P99_Count', 'Concentration']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    metric_data = df_changes[df_changes['Metric'] == metric].copy()\n",
    "    \n",
    "    # Create matrix\n",
    "    matrix = metric_data.set_index('Demographic_EN')[['Q1_Q2_Change', 'Q2_Q3_Change', 'Q1_Q3_Change']]\n",
    "    matrix = matrix.reindex(demo_en_ordered)\n",
    "    matrix.columns = ['Q1→Q2', 'Q2→Q3', 'Q1→Q3']\n",
    "    \n",
    "    # Determine color limits symmetrically\n",
    "    vmax = max(abs(matrix.values.min()), abs(matrix.values.max()))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        vmin=-vmax,\n",
    "        vmax=vmax,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Change (%)'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 10}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(metric_labels[metric], fontsize=12, pad=10)\n",
    "    ax.set_xlabel('Layer Transition', fontsize=11)\n",
    "    ax.set_ylabel('Demographic', fontsize=11)\n",
    "\n",
    "plt.suptitle('Cross-Layer Change Ratios\\n(Red=Increase, Blue=Decrease)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_layer_changes_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Cross-Demographic Comparison (Within-Layer)\n",
    "\n",
    "### How It's Computed:\n",
    "For each layer, we compute Jaccard similarity between all pairs of demographics: J(A,B) = |A∩B| / |A∪B|, where A and B are sets of important feature indices (99th percentile). Higher similarity means demographics share more bias-encoding features.\n",
    "\n",
    "### Analysis:\n",
    "High Jaccard similarity (close to 1) between demographics suggests shared bias mechanisms - intervening on these features could reduce bias across multiple dimensions. Low similarity indicates demographic-specific bias encoding, requiring targeted interventions for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(set_a, set_b):\n",
    "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
    "    set_a = set(set_a)\n",
    "    set_b = set(set_b)\n",
    "    \n",
    "    intersection = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# Compute Jaccard similarity matrices for each layer\n",
    "jaccard_matrices = {}\n",
    "\n",
    "for lq in LAYER_QUANTILES:\n",
    "    n_demo = len(DEMOGRAPHICS)\n",
    "    matrix = np.zeros((n_demo, n_demo))\n",
    "    \n",
    "    for i, demo_i in enumerate(DEMOGRAPHICS):\n",
    "        for j, demo_j in enumerate(DEMOGRAPHICS):\n",
    "            if i == j:\n",
    "                matrix[i, j] = 1.0\n",
    "            else:\n",
    "                # Get important features for both demographics\n",
    "                row_i = df_stats[(df_stats['Demographic'] == demo_i) & (df_stats['Layer'] == lq)]\n",
    "                row_j = df_stats[(df_stats['Demographic'] == demo_j) & (df_stats['Layer'] == lq)]\n",
    "                \n",
    "                if len(row_i) > 0 and len(row_j) > 0:\n",
    "                    indices_i = row_i['P99_Indices'].values[0]\n",
    "                    indices_j = row_j['P99_Indices'].values[0]\n",
    "                    matrix[i, j] = compute_jaccard_similarity(indices_i, indices_j)\n",
    "    \n",
    "    jaccard_matrices[lq] = matrix\n",
    "\n",
    "print(\"Jaccard similarity matrices computed for each layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Jaccard similarity heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "demo_en_labels = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "\n",
    "for idx, lq in enumerate(LAYER_QUANTILES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    matrix = jaccard_matrices[lq]\n",
    "    \n",
    "    # Create DataFrame for better labeling\n",
    "    df_matrix = pd.DataFrame(matrix, index=demo_en_labels, columns=demo_en_labels)\n",
    "    \n",
    "    # Mask diagonal for cleaner visualization\n",
    "    mask = np.eye(len(DEMOGRAPHICS), dtype=bool)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        df_matrix,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='YlGnBu',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=ax,\n",
    "        mask=mask,\n",
    "        cbar_kws={'label': 'Jaccard Similarity'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 8}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'{LAYER_LABELS[lq]}', fontsize=12, pad=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.suptitle('Feature Overlap Between Demographics (99th Percentile)\\nJaccard Similarity: |A∩B| / |A∪B|', \n",
    "             fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_jaccard_similarity_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print average similarity per layer\n",
    "print(\"\\nAverage Jaccard Similarity (excluding diagonal):\")\n",
    "for lq in LAYER_QUANTILES:\n",
    "    matrix = jaccard_matrices[lq]\n",
    "    # Get off-diagonal elements\n",
    "    off_diag = matrix[~np.eye(matrix.shape[0], dtype=bool)]\n",
    "    print(f\"  {LAYER_LABELS[lq]}: {off_diag.mean():.3f} (std={off_diag.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shared vs unique features\n",
    "shared_features_data = []\n",
    "\n",
    "for lq in LAYER_QUANTILES:\n",
    "    # Collect all important features for this layer\n",
    "    all_features = {}\n",
    "    for demo in DEMOGRAPHICS:\n",
    "        row = df_stats[(df_stats['Demographic'] == demo) & (df_stats['Layer'] == lq)]\n",
    "        if len(row) > 0:\n",
    "            all_features[demo] = set(row['P99_Indices'].values[0])\n",
    "    \n",
    "    # Count how many demographics each feature appears in\n",
    "    feature_counts = defaultdict(int)\n",
    "    for demo, features in all_features.items():\n",
    "        for f in features:\n",
    "            feature_counts[f] += 1\n",
    "    \n",
    "    # Group by count\n",
    "    count_distribution = defaultdict(int)\n",
    "    for f, count in feature_counts.items():\n",
    "        count_distribution[count] += 1\n",
    "    \n",
    "    for count, n_features in count_distribution.items():\n",
    "        shared_features_data.append({\n",
    "            'Layer': lq,\n",
    "            'Layer_Label': LAYER_LABELS[lq],\n",
    "            'Shared_By_N_Demographics': count,\n",
    "            'Feature_Count': n_features\n",
    "        })\n",
    "\n",
    "df_shared = pd.DataFrame(shared_features_data)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Pivot for grouped bar chart\n",
    "pivot = df_shared.pivot(index='Shared_By_N_Demographics', columns='Layer_Label', values='Feature_Count')\n",
    "pivot = pivot.fillna(0).astype(int)\n",
    "\n",
    "pivot.plot(kind='bar', ax=ax, width=0.8, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Number of Demographics Sharing the Feature', fontsize=12)\n",
    "ax.set_ylabel('Number of Features', fontsize=12)\n",
    "ax.set_title('Feature Sharing Distribution (99th Percentile)\\nHow Many Demographics Share Each Important Feature?', \n",
    "             fontsize=13, pad=15)\n",
    "ax.legend(title='Layer')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_sharing_distribution_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFeature Sharing Summary:\")\n",
    "for lq in LAYER_QUANTILES:\n",
    "    lq_data = df_shared[df_shared['Layer'] == lq]\n",
    "    total = lq_data['Feature_Count'].sum()\n",
    "    unique = lq_data[lq_data['Shared_By_N_Demographics'] == 1]['Feature_Count'].sum()\n",
    "    shared_3plus = lq_data[lq_data['Shared_By_N_Demographics'] >= 3]['Feature_Count'].sum()\n",
    "    print(f\"  {LAYER_LABELS[lq]}: {total} total features, {unique} unique (1 demo), {shared_3plus} shared (3+ demos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering of demographics based on feature overlap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, lq in enumerate(LAYER_QUANTILES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Convert Jaccard similarity to distance (1 - similarity)\n",
    "    similarity = jaccard_matrices[lq]\n",
    "    distance = 1 - similarity\n",
    "    \n",
    "    # Ensure diagonal is 0\n",
    "    np.fill_diagonal(distance, 0)\n",
    "    \n",
    "    # Convert to condensed form for linkage\n",
    "    condensed = squareform(distance)\n",
    "    \n",
    "    # Compute linkage\n",
    "    linkage_matrix = linkage(condensed, method='average')\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    dendrogram(\n",
    "        linkage_matrix,\n",
    "        labels=demo_en_labels,\n",
    "        ax=ax,\n",
    "        leaf_rotation=45,\n",
    "        leaf_font_size=10\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'{LAYER_LABELS[lq]}', fontsize=12, pad=10)\n",
    "    ax.set_ylabel('Distance (1 - Jaccard)', fontsize=11)\n",
    "\n",
    "plt.suptitle('Demographic Clustering by Feature Similarity\\n(Lower distance = more similar important features)', \n",
    "             fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_demographic_clustering_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Important Feature Deep Dive\n",
    "\n",
    "### How It's Computed:\n",
    "For each demographic-layer combination, we extract the top-10 features by IG² score, tracking their indices and scores. We also analyze which features appear as \"top\" across multiple demographics to identify potentially universal bias features.\n",
    "\n",
    "### Analysis:\n",
    "Features appearing in top-10 across multiple demographics represent \"universal bias\" features - high-impact targets for debiasing. Features unique to specific demographics indicate demographic-specific bias encoding patterns requiring targeted intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top-10 features for each demographic-layer\n",
    "top_features_data = []\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    demo_en = DEMOGRAPHIC_EN[demo]\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        if lq in ig2_data[demo]:\n",
    "            scores = ig2_data[demo][lq]['scores']\n",
    "            \n",
    "            # Get top-10 indices\n",
    "            top_indices = np.argsort(scores)[-10:][::-1]\n",
    "            \n",
    "            for rank, idx in enumerate(top_indices, 1):\n",
    "                top_features_data.append({\n",
    "                    'Demographic': demo,\n",
    "                    'Demographic_EN': demo_en,\n",
    "                    'Layer': lq,\n",
    "                    'Rank': rank,\n",
    "                    'Feature_Index': int(idx),\n",
    "                    'IG2_Score': scores[idx]\n",
    "                })\n",
    "\n",
    "df_top_features = pd.DataFrame(top_features_data)\n",
    "print(f\"Extracted {len(df_top_features)} top-10 feature entries\")\n",
    "print(\"\\nSample:\")\n",
    "print(df_top_features.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features that appear in top-10 across multiple demographics (per layer)\n",
    "universal_features = []\n",
    "\n",
    "for lq in LAYER_QUANTILES:\n",
    "    layer_data = df_top_features[df_top_features['Layer'] == lq]\n",
    "    \n",
    "    # Count appearances of each feature\n",
    "    feature_counts = layer_data.groupby('Feature_Index').agg({\n",
    "        'Demographic_EN': lambda x: list(x),\n",
    "        'IG2_Score': 'mean',\n",
    "        'Rank': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    feature_counts['N_Demographics'] = feature_counts['Demographic_EN'].apply(len)\n",
    "    feature_counts['Layer'] = lq\n",
    "    \n",
    "    # Filter to features appearing in 2+ demographics\n",
    "    multi_demo = feature_counts[feature_counts['N_Demographics'] >= 2].copy()\n",
    "    multi_demo = multi_demo.sort_values('N_Demographics', ascending=False)\n",
    "    \n",
    "    universal_features.append(multi_demo)\n",
    "\n",
    "df_universal = pd.concat(universal_features, ignore_index=True)\n",
    "\n",
    "print(\"Features appearing in top-10 across multiple demographics:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for lq in LAYER_QUANTILES:\n",
    "    lq_data = df_universal[df_universal['Layer'] == lq]\n",
    "    print(f\"\\n{LAYER_LABELS[lq]}:\")\n",
    "    if len(lq_data) > 0:\n",
    "        for _, row in lq_data.head(5).iterrows():\n",
    "            demos = ', '.join(row['Demographic_EN'][:3])\n",
    "            if len(row['Demographic_EN']) > 3:\n",
    "                demos += f' +{len(row[\"Demographic_EN\"])-3} more'\n",
    "            print(f\"  Feature #{row['Feature_Index']:5d}: {row['N_Demographics']} demographics, \"\n",
    "                  f\"avg score={row['IG2_Score']:.4f} [{demos}]\")\n",
    "    else:\n",
    "        print(\"  No features shared across multiple demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Feature-Demographic heatmap for top shared features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "for idx, lq in enumerate(LAYER_QUANTILES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get top 20 most frequently appearing features for this layer\n",
    "    layer_data = df_top_features[df_top_features['Layer'] == lq]\n",
    "    feature_counts = layer_data['Feature_Index'].value_counts().head(20)\n",
    "    top_feature_indices = feature_counts.index.tolist()\n",
    "    \n",
    "    # Create matrix: features x demographics\n",
    "    matrix = np.zeros((len(top_feature_indices), len(DEMOGRAPHICS)))\n",
    "    \n",
    "    for i, feat_idx in enumerate(top_feature_indices):\n",
    "        for j, demo in enumerate(DEMOGRAPHICS):\n",
    "            if lq in ig2_data[demo]:\n",
    "                scores = ig2_data[demo][lq]['scores']\n",
    "                matrix[i, j] = scores[feat_idx]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_matrix = pd.DataFrame(\n",
    "        matrix,\n",
    "        index=[f'#{idx}' for idx in top_feature_indices],\n",
    "        columns=[DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(\n",
    "        df_matrix,\n",
    "        cmap='YlOrRd',\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'IG² Score'},\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'{LAYER_LABELS[lq]}\\nTop 20 Most Shared Features', fontsize=12, pad=10)\n",
    "    ax.set_xlabel('Demographic', fontsize=11)\n",
    "    ax.set_ylabel('Feature Index', fontsize=11)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('IG² Scores of Top Shared Features Across Demographics', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"important_neurons_feature_demographic_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Summary Statistics & Export\n",
    "\n",
    "### Purpose:\n",
    "Aggregate all findings into exportable summary tables for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary DataFrame\n",
    "summary_cols = ['Demographic_EN', 'Layer', 'Max_Score', 'Max_Neuron_Idx', \n",
    "                'Total_Nonzero', 'P95_Count', 'P99_Count', \n",
    "                'Concentration_Top10', 'Concentration_Top50']\n",
    "\n",
    "df_export = df_stats[summary_cols].copy()\n",
    "df_export.columns = ['Demographic', 'Layer', 'Max_IG2_Score', 'Top_Neuron_Index',\n",
    "                     'Nonzero_Features', 'P95_Count', 'P99_Count',\n",
    "                     'Concentration_Top10', 'Concentration_Top50']\n",
    "\n",
    "# Save to CSV\n",
    "export_path = ASSETS_DIR / f\"important_neurons_summary_{STAGE}.csv\"\n",
    "df_export.to_csv(export_path, index=False)\n",
    "print(f\"Summary exported to: {export_path}\")\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"IMPORTANT NEURONS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(df_export.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top features list\n",
    "top_features_export = df_top_features[['Demographic_EN', 'Layer', 'Rank', 'Feature_Index', 'IG2_Score']].copy()\n",
    "top_features_export.columns = ['Demographic', 'Layer', 'Rank', 'Feature_Index', 'IG2_Score']\n",
    "\n",
    "export_path_features = ASSETS_DIR / f\"important_neurons_top_features_{STAGE}.csv\"\n",
    "top_features_export.to_csv(export_path_features, index=False)\n",
    "print(f\"Top features exported to: {export_path_features}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Feature Concentration:\")\n",
    "for lq in LAYER_QUANTILES:\n",
    "    lq_data = df_stats[df_stats['Layer'] == lq]\n",
    "    avg_conc = lq_data['Concentration_Top10'].mean()\n",
    "    print(f\"   {LAYER_LABELS[lq]}: Top-10 features capture {avg_conc*100:.1f}% of total IG² score (avg)\")\n",
    "\n",
    "print(\"\\n2. Cross-Demographic Similarity:\")\n",
    "for lq in LAYER_QUANTILES:\n",
    "    matrix = jaccard_matrices[lq]\n",
    "    off_diag = matrix[~np.eye(matrix.shape[0], dtype=bool)]\n",
    "    print(f\"   {LAYER_LABELS[lq]}: Average Jaccard similarity = {off_diag.mean():.3f}\")\n",
    "\n",
    "print(\"\\n3. Universal Features (appearing in 3+ demographics' top-10):\")\n",
    "for lq in LAYER_QUANTILES:\n",
    "    lq_data = df_universal[(df_universal['Layer'] == lq) & (df_universal['N_Demographics'] >= 3)]\n",
    "    print(f\"   {LAYER_LABELS[lq]}: {len(lq_data)} universal features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Important Neuron Analysis Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAssets saved to: {ASSETS_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(ASSETS_DIR.glob(f\"important_neurons*_{STAGE}*\")):\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
