{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Feature Activation Heatmaps\n",
    "\n",
    "This notebook visualizes feature activation patterns across bias prompts.\n",
    "\n",
    "**Purpose:**\n",
    "- Show which features activate for which types of prompts\n",
    "- Identify patterns in feature activation across demographics\n",
    "- Visualize sparsity and activation magnitudes\n",
    "\n",
    "**Input Data:**\n",
    "- SAE feature activations: [N_prompts × 100,000]\n",
    "- Top bias features from IG² attribution\n",
    "\n",
    "**Output:**\n",
    "- Heatmap of activations [prompts × features]\n",
    "- Sparsity analysis\n",
    "- Clustering visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path (works from notebooks/visualizations/)\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.visualization import (\n",
    "    setup_korean_font,\n",
    "    ensure_korean_font,\n",
    "    load_demographics,\n",
    "    load_ig2_results,\n",
    "    load_activations,\n",
    "    load_sae_decoder_weights,\n",
    "    plot_activation_heatmap,\n",
    "    select_top_features_union,\n",
    "    get_demographic_labels,\n",
    "    get_available_demographics\n",
    ")\n",
    "from src.models.sae import GatedAutoEncoder, AutoEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Notebook dir: {NOTEBOOK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Korean font for matplotlib (improved version with auto-detection)\n",
    "font_name = ensure_korean_font()\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "ASSETS_DIR = PROJECT_ROOT / \"notebooks\" / \"visualizations\" / \"assets\"\n",
    "ASSETS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stage: 'pilot', 'medium', or 'full'\n",
    "STAGE = \"pilot\"\n",
    "\n",
    "# SAE configuration\n",
    "SAE_TYPE = \"gated\"  # 'standard' or 'gated'\n",
    "LAYER_QUANTILE = \"q2\"  # 'q1', 'q2', or 'q3'\n",
    "\n",
    "# Demographic to analyze (None = use first available)\n",
    "DEMOGRAPHIC = None  # e.g., \"성별\", \"인종\", etc.\n",
    "\n",
    "# Visualization parameters\n",
    "TOP_K = 50  # Top features per demographic\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"\\nStage: {STAGE}\")\n",
    "print(f\"SAE type: {SAE_TYPE}\")\n",
    "print(f\"Layer quantile: {LAYER_QUANTILE}\")\n",
    "print(f\"Demographic: {DEMOGRAPHIC or 'auto-select'}\")\n",
    "print(f\"Top-K: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographics\n",
    "demographics_dict = load_demographics(DATA_DIR)\n",
    "demographic_labels_ko, _ = get_demographic_labels(demographics_dict)\n",
    "\n",
    "# Get available demographics with results\n",
    "available_demographics = get_available_demographics(RESULTS_DIR, stage=STAGE)\n",
    "print(f\"Available demographics: {available_demographics}\")\n",
    "\n",
    "# Load IG² results (loads from all per-demographic directories)\n",
    "ig2_results = load_ig2_results(RESULTS_DIR, stage=STAGE, demographic=DEMOGRAPHIC)\n",
    "\n",
    "print(f\"\\nIG² results loaded for {len(ig2_results)} demographics\")\n",
    "for demo in ig2_results.keys():\n",
    "    print(f\"  - {demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Activations and Compute SAE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations from pickle file\n",
    "# If DEMOGRAPHIC is specified, load from per-demographic directory\n",
    "if DEMOGRAPHIC:\n",
    "    selected_demo = DEMOGRAPHIC\n",
    "else:\n",
    "    # Use first available demographic\n",
    "    selected_demo = list(ig2_results.keys())[0]\n",
    "    print(f\"Auto-selected demographic: {selected_demo}\")\n",
    "\n",
    "activations, labels, prompts = load_activations(\n",
    "    RESULTS_DIR, stage=STAGE, demographic=selected_demo, layer_quantile=LAYER_QUANTILE\n",
    ")\n",
    "\n",
    "print(f\"\\nActivations shape: {activations.shape}\")\n",
    "print(f\"Number of samples: {len(labels)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label in set(labels):\n",
    "    count = labels.count(label) if isinstance(labels, list) else (labels == label).sum().item()\n",
    "    print(f\"  - {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAE model and compute features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sae_path = RESULTS_DIR / \"models\" / f\"sae-{SAE_TYPE}_{STAGE}_{LAYER_QUANTILE}\" / \"model.pth\"\n",
    "\n",
    "if not sae_path.exists():\n",
    "    raise FileNotFoundError(f\"SAE model not found at {sae_path}. Please run 03_train_sae.py first.\")\n",
    "\n",
    "print(f\"Loading SAE from {sae_path}\")\n",
    "if SAE_TYPE == 'gated':\n",
    "    sae = GatedAutoEncoder.from_pretrained(str(sae_path))\n",
    "else:\n",
    "    sae = AutoEncoder.from_pretrained(str(sae_path))\n",
    "\n",
    "sae.to(device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded: dict_size={sae.dict_size}, activation_dim={sae.activation_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SAE features from activations\n",
    "n_samples = len(activations)\n",
    "features = torch.zeros(n_samples, sae.dict_size)\n",
    "\n",
    "print(f\"Extracting SAE features for {n_samples} samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(n_samples), desc=\"Extracting features\"):\n",
    "        sample = activations[i].unsqueeze(0).to(device)\n",
    "        _, feat = sae(sample, output_features=True)\n",
    "        features[i] = feat[0].cpu()\n",
    "\n",
    "# Create prompt_ids\n",
    "prompt_ids = [f\"prompt_{i}\" for i in range(n_samples)]\n",
    "\n",
    "print(f\"\\nFeature activations shape: {features.shape}\")\n",
    "print(f\"Number of prompts: {len(prompt_ids)}\")\n",
    "\n",
    "# Clean up SAE from GPU memory\n",
    "del sae\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Select Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select union of top features across demographics\n",
    "all_top_features, demographic2topfeatures = select_top_features_union(\n",
    "    ig2_results, top_k=TOP_K\n",
    ")\n",
    "\n",
    "print(f\"Total unique features selected: {len(all_top_features)}\")\n",
    "print(f\"Features per demographic: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Plot Activation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "fig = plot_activation_heatmap(\n",
    "    feature_activations=features,\n",
    "    selected_features=all_top_features,\n",
    "    prompt_labels=[f\"P{i}\" for i in range(len(features))],\n",
    "    save_path=ASSETS_DIR / f\"activation_heatmap_{STAGE}_top{TOP_K}.png\",\n",
    "    figsize=(20, 12),\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "from src.visualization.feature_selection import compute_feature_sparsity\n\n# Compute sparsity\noverall_sparsity, per_feature_sparsity = compute_feature_sparsity(features)\n\nprint(f\"Overall sparsity: {overall_sparsity:.2%}\")\nprint(f\"Mean per-feature sparsity: {per_feature_sparsity.mean():.2%}\")\nprint(f\"Median per-feature sparsity: {per_feature_sparsity.median():.2%}\")\n\n# Plot sparsity distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram of per-feature sparsity\naxes[0].hist(per_feature_sparsity.cpu().numpy(), bins=50, color='skyblue', edgecolor='black')\naxes[0].axvline(overall_sparsity, color='red', linestyle='--', label=f'Overall: {overall_sparsity:.2%}')\naxes[0].set_xlabel('Sparsity (fraction of zeros)', fontsize=12)\naxes[0].set_ylabel('Number of Features', fontsize=12)\naxes[0].set_title('Per-Feature Sparsity Distribution', fontsize=14)\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Activation magnitude distribution (non-zero only)\nnon_zero_activations = features[features > 0]\naxes[1].hist(non_zero_activations.cpu().numpy(), bins=50, color='lightcoral', edgecolor='black')\naxes[1].set_xlabel('Activation Magnitude', fontsize=12)\naxes[1].set_ylabel('Frequency', fontsize=12)\naxes[1].set_title('Activation Magnitude Distribution (non-zero)', fontsize=14)\naxes[1].set_yscale('log')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(ASSETS_DIR / f\"sparsity_analysis_{STAGE}.png\", dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Feature Activation Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Compute activation frequency for top features\nselected_features = features[:, all_top_features]\nactivation_freq = (selected_features > 0).float().mean(dim=0)\n\n# Plot\nfig, ax = plt.subplots(figsize=(14, 5))\n\nx = np.arange(len(all_top_features))\nax.bar(x, activation_freq.cpu().numpy(), color='seagreen', alpha=0.7)\nax.set_xlabel('Feature Index (sorted by selection)', fontsize=12)\nax.set_ylabel('Activation Frequency', fontsize=12)\nax.set_title('Bias Feature Activation Frequency', fontsize=14, pad=15)\nax.grid(axis='y', alpha=0.3)\n\n# Add mean line\nax.axhline(activation_freq.mean().item(), color='red', linestyle='--', \n           label=f'Mean: {activation_freq.mean():.2%}')\nax.legend()\n\nplt.tight_layout()\nplt.savefig(ASSETS_DIR / f\"activation_frequency_{STAGE}_top{TOP_K}.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nActivation Frequency Statistics:\")\nprint(f\"  Mean: {activation_freq.mean():.2%}\")\nprint(f\"  Median: {activation_freq.median():.2%}\")\nprint(f\"  Min: {activation_freq.min():.2%}\")\nprint(f\"  Max: {activation_freq.max():.2%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Clustered Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Cluster prompts by activation patterns\nn_clusters = min(5, len(features) // 5 + 1)  # Adjust for small sample sizes\n\nif len(features) >= n_clusters and n_clusters > 1:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    cluster_labels = kmeans.fit_predict(selected_features.cpu().numpy())\n    \n    # Sort by cluster\n    sort_idx = np.argsort(cluster_labels)\n    sorted_features = selected_features[sort_idx]\n    sorted_labels = cluster_labels[sort_idx]\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=(20, 12))\n    \n    im = ax.imshow(sorted_features.cpu().numpy(), aspect='auto', cmap='viridis', interpolation='nearest')\n    \n    # Add cluster boundaries\n    boundaries = [0]\n    for i in range(1, len(sorted_labels)):\n        if sorted_labels[i] != sorted_labels[i-1]:\n            boundaries.append(i)\n            ax.axhline(i - 0.5, color='red', linewidth=2, alpha=0.7)\n    boundaries.append(len(sorted_labels))\n    \n    # Add cluster labels\n    for i in range(len(boundaries) - 1):\n        mid = (boundaries[i] + boundaries[i+1]) / 2\n        ax.text(-5, mid, f'C{i}', va='center', ha='right', fontsize=12, fontweight='bold')\n    \n    plt.colorbar(im, ax=ax, label='Activation Magnitude')\n    ax.set_xlabel(f'Top {TOP_K} Features per Demographic', fontsize=14)\n    ax.set_ylabel('Prompts (clustered)', fontsize=14)\n    ax.set_title('Clustered Feature Activation Patterns', fontsize=16, pad=20)\n    \n    plt.tight_layout()\n    plt.savefig(ASSETS_DIR / f\"clustered_activation_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nClustered {len(features)} prompts into {n_clusters} groups\")\n    for i in range(n_clusters):\n        count = (cluster_labels == i).sum()\n        print(f\"  Cluster {i}: {count} prompts\")\nelse:\n    print(f\"Not enough prompts for clustering (have {len(features)}, need at least {n_clusters})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Activation Patterns:**\n",
    "   - Are certain features consistently active across prompts?\n",
    "   - Are there prompt-specific feature activations?\n",
    "   - Do features cluster together in activation?\n",
    "\n",
    "2. **Sparsity:**\n",
    "   - Is the target sparsity (95-99%) achieved?\n",
    "   - Are some features more sparse than others?\n",
    "   - What is the distribution of non-zero activations?\n",
    "\n",
    "3. **Clustering:**\n",
    "   - Do prompts group by demographic dimension?\n",
    "   - Are there distinct activation profiles?\n",
    "   - Which features are shared vs. unique across clusters?\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test manipulation effects on high-frequency features\n",
    "2. Investigate features with unusual activation patterns\n",
    "3. Correlate activation patterns with bias scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}