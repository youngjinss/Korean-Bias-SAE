{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SAE Training Loss Visualization\n",
    "\n",
    "This notebook visualizes the training progress and loss curves for the Sparse Autoencoder (SAE).\n",
    "\n",
    "**Purpose:**\n",
    "- Monitor training convergence\n",
    "- Analyze reconstruction vs. sparsity loss trade-off\n",
    "- Verify target sparsity is achieved\n",
    "\n",
    "**Input Data:**\n",
    "- Training logs CSV: step, total_loss, recon_loss, sparsity_loss, sparsity_l0\n",
    "\n",
    "**Output:**\n",
    "- Training loss curves (total, reconstruction, sparsity)\n",
    "- Sparsity (L0) progression\n",
    "- Convergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to path (works from notebooks/visualizations/)\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.visualization import (\n",
    "    setup_korean_font,\n",
    "    ensure_korean_font,\n",
    "    load_training_logs,\n",
    "    plot_training_loss\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Notebook dir: {NOTEBOOK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Korean font for matplotlib (improved version with auto-detection)\n",
    "font_name = ensure_korean_font()\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "ASSETS_DIR = PROJECT_ROOT / \"notebooks\" / \"visualizations\" / \"assets\"\n",
    "ASSETS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stage: 'pilot', 'medium', or 'full'\n",
    "STAGE = \"pilot\"\n",
    "\n",
    "# SAE configuration\n",
    "SAE_TYPE = \"gated\"  # 'standard' or 'gated'\n",
    "LAYER_QUANTILE = \"q2\"  # 'q1', 'q2', or 'q3'\n",
    "\n",
    "# Target sparsity (for reference)\n",
    "TARGET_SPARSITY_L0 = 0.05  # Target L0 sparsity (5% active features)\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"\\nStage: {STAGE}\")\n",
    "print(f\"SAE type: {SAE_TYPE}\")\n",
    "print(f\"Layer quantile: {LAYER_QUANTILE}\")\n",
    "print(f\"Target sparsity (L0): {TARGET_SPARSITY_L0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs\n",
    "training_logs = load_training_logs(\n",
    "    RESULTS_DIR, \n",
    "    stage=STAGE, \n",
    "    sae_type=SAE_TYPE,\n",
    "    layer_quantile=LAYER_QUANTILE\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(training_logs)} training steps\")\n",
    "print(f\"\\nColumns: {list(training_logs.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(training_logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Plot Training Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_loss(\n",
    "    training_logs=training_logs,\n",
    "    save_path=ASSETS_DIR / f\"sae_training_loss_{STAGE}_{SAE_TYPE}_{LAYER_QUANTILE}.png\",\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Detailed Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced loss visualization with smoothing\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.flatten()\n\n# Window for smoothing\nwindow_size = max(1, len(training_logs) // 50)\n\ndef smooth(series, window=window_size):\n    return series.rolling(window=window, min_periods=1).mean()\n\n# Total loss\nif 'total_loss' in training_logs.columns:\n    ax = axes[0]\n    ax.plot(training_logs['step'], training_logs['total_loss'], alpha=0.3, color='blue', label='Raw')\n    ax.plot(training_logs['step'], smooth(training_logs['total_loss']), color='blue', linewidth=2, label='Smoothed')\n    ax.set_xlabel('Training Step', fontsize=12)\n    ax.set_ylabel('Loss', fontsize=12)\n    ax.set_title('Total Loss', fontsize=14, pad=10)\n    ax.legend()\n    ax.grid(alpha=0.3)\n\n# Reconstruction loss\nif 'recon_loss' in training_logs.columns:\n    ax = axes[1]\n    ax.plot(training_logs['step'], training_logs['recon_loss'], alpha=0.3, color='green', label='Raw')\n    ax.plot(training_logs['step'], smooth(training_logs['recon_loss']), color='green', linewidth=2, label='Smoothed')\n    ax.set_xlabel('Training Step', fontsize=12)\n    ax.set_ylabel('Loss', fontsize=12)\n    ax.set_title('Reconstruction Loss', fontsize=14, pad=10)\n    ax.legend()\n    ax.grid(alpha=0.3)\n\n# Sparsity loss\nif 'sparsity_loss' in training_logs.columns:\n    ax = axes[2]\n    ax.plot(training_logs['step'], training_logs['sparsity_loss'], alpha=0.3, color='red', label='Raw')\n    ax.plot(training_logs['step'], smooth(training_logs['sparsity_loss']), color='red', linewidth=2, label='Smoothed')\n    ax.set_xlabel('Training Step', fontsize=12)\n    ax.set_ylabel('Loss', fontsize=12)\n    ax.set_title('Sparsity Loss', fontsize=14, pad=10)\n    ax.legend()\n    ax.grid(alpha=0.3)\n\n# Sparsity L0 with target line\nif 'sparsity_l0' in training_logs.columns:\n    ax = axes[3]\n    ax.plot(training_logs['step'], training_logs['sparsity_l0'], alpha=0.3, color='purple', label='Raw')\n    ax.plot(training_logs['step'], smooth(training_logs['sparsity_l0']), color='purple', linewidth=2, label='Smoothed')\n    ax.axhline(TARGET_SPARSITY_L0, color='orange', linestyle='--', linewidth=2, label=f'Target: {TARGET_SPARSITY_L0}')\n    ax.set_xlabel('Training Step', fontsize=12)\n    ax.set_ylabel('L0 (Active Features Ratio)', fontsize=12)\n    ax.set_title('Sparsity (L0)', fontsize=14, pad=10)\n    ax.legend()\n    ax.grid(alpha=0.3)\n\nplt.suptitle(f'SAE Training Progress ({SAE_TYPE}, {STAGE}, {LAYER_QUANTILE})', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.savefig(ASSETS_DIR / f\"sae_training_detailed_{STAGE}_{SAE_TYPE}_{LAYER_QUANTILE}.png\", \n            dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final values (average of last 10%)\n",
    "last_n = max(1, len(training_logs) // 10)\n",
    "final_metrics = training_logs.tail(last_n)\n",
    "\n",
    "print(f\"\\nTotal training steps: {len(training_logs)}\")\n",
    "print(f\"\\nFinal metrics (average of last {last_n} steps):\")\n",
    "\n",
    "metric_names = {\n",
    "    'total_loss': 'Total Loss',\n",
    "    'recon_loss': 'Reconstruction Loss',\n",
    "    'sparsity_loss': 'Sparsity Loss',\n",
    "    'sparsity_l0': 'Sparsity (L0)'\n",
    "}\n",
    "\n",
    "for col, name in metric_names.items():\n",
    "    if col in training_logs.columns:\n",
    "        final_mean = final_metrics[col].mean()\n",
    "        final_std = final_metrics[col].std()\n",
    "        initial = training_logs[col].iloc[0] if len(training_logs) > 0 else 0\n",
    "        \n",
    "        if col == 'sparsity_l0':\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Initial:   {initial:.4f}\")\n",
    "            print(f\"  Final:     {final_mean:.4f} +/- {final_std:.4f}\")\n",
    "            print(f\"  Target:    {TARGET_SPARSITY_L0}\")\n",
    "            if TARGET_SPARSITY_L0 > 0:\n",
    "                deviation = abs(final_mean - TARGET_SPARSITY_L0) / TARGET_SPARSITY_L0 * 100\n",
    "                print(f\"  Deviation: {deviation:.1f}%\")\n",
    "        else:\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Initial:   {initial:.6f}\")\n",
    "            print(f\"  Final:     {final_mean:.6f} +/- {final_std:.6f}\")\n",
    "            if initial > 0:\n",
    "                reduction = (1 - final_mean / initial) * 100\n",
    "                print(f\"  Reduction: {reduction:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence\n",
    "if 'total_loss' in training_logs.columns and len(training_logs) > 10:\n",
    "    # Divide training into segments\n",
    "    n_segments = 5\n",
    "    segment_size = len(training_logs) // n_segments\n",
    "    \n",
    "    print(\"\\nConvergence Analysis (Total Loss):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    segment_means = []\n",
    "    for i in range(n_segments):\n",
    "        start = i * segment_size\n",
    "        end = (i + 1) * segment_size if i < n_segments - 1 else len(training_logs)\n",
    "        segment = training_logs.iloc[start:end]\n",
    "        mean_loss = segment['total_loss'].mean()\n",
    "        segment_means.append(mean_loss)\n",
    "        \n",
    "        # Calculate percentage of training\n",
    "        pct_start = start / len(training_logs) * 100\n",
    "        pct_end = end / len(training_logs) * 100\n",
    "        print(f\"  Segment {i+1} ({pct_start:.0f}%-{pct_end:.0f}%): Loss = {mean_loss:.6f}\")\n",
    "    \n",
    "    # Check if converging (loss should be decreasing or stable)\n",
    "    final_vs_initial = (segment_means[-1] - segment_means[0]) / segment_means[0] * 100\n",
    "    final_vs_mid = (segment_means[-1] - segment_means[n_segments//2]) / segment_means[n_segments//2] * 100\n",
    "    \n",
    "    print(f\"\\n  Total change: {final_vs_initial:+.2f}%\")\n",
    "    print(f\"  Change in 2nd half: {final_vs_mid:+.2f}%\")\n",
    "    \n",
    "    if abs(final_vs_mid) < 5:\n",
    "        print(\"\\n  Status: Converged (stable)\")\n",
    "    elif final_vs_mid < 0:\n",
    "        print(\"\\n  Status: Still improving\")\n",
    "    else:\n",
    "        print(\"\\n  Status: Potential overfitting (loss increasing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Loss Component Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze balance between reconstruction and sparsity\nif 'recon_loss' in training_logs.columns and 'sparsity_loss' in training_logs.columns:\n    fig, ax = plt.subplots(figsize=(12, 5))\n    \n    # Normalize losses for comparison\n    recon_norm = training_logs['recon_loss'] / training_logs['recon_loss'].max()\n    sparse_norm = training_logs['sparsity_loss'] / training_logs['sparsity_loss'].max()\n    \n    ax.plot(training_logs['step'], smooth(recon_norm), color='green', linewidth=2, label='Reconstruction (normalized)')\n    ax.plot(training_logs['step'], smooth(sparse_norm), color='red', linewidth=2, label='Sparsity (normalized)')\n    \n    ax.set_xlabel('Training Step', fontsize=12)\n    ax.set_ylabel('Normalized Loss', fontsize=12)\n    ax.set_title('Reconstruction vs Sparsity Loss Balance', fontsize=14, pad=10)\n    ax.legend(fontsize=11)\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(ASSETS_DIR / f\"loss_balance_{STAGE}_{SAE_TYPE}_{LAYER_QUANTILE}.png\", \n                dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Compute ratio\n    final_recon = final_metrics['recon_loss'].mean()\n    final_sparse = final_metrics['sparsity_loss'].mean()\n    if final_sparse > 0:\n        ratio = final_recon / final_sparse\n        print(f\"\\nFinal recon/sparsity ratio: {ratio:.2f}\")\n        print(f\"  (>1 means reconstruction dominates, <1 means sparsity dominates)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Sparsity Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "if 'sparsity_l0' in training_logs.columns:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Sparsity over time\n    ax = axes[0]\n    ax.plot(training_logs['step'], training_logs['sparsity_l0'], linewidth=2, color='purple')\n    ax.axhline(TARGET_SPARSITY_L0, color='red', linestyle='--', \n               label=f'Target ({TARGET_SPARSITY_L0:.1%})', alpha=0.7)\n    ax.set_xlabel('Step', fontsize=12)\n    ax.set_ylabel('Sparsity (L0)', fontsize=12)\n    ax.set_title('Sparsity Evolution', fontsize=14, pad=10)\n    ax.legend(fontsize=11)\n    ax.grid(alpha=0.3)\n    \n    # Distribution of sparsity\n    ax = axes[1]\n    ax.hist(training_logs['sparsity_l0'], bins=50, color='purple', alpha=0.7, edgecolor='black')\n    ax.axvline(training_logs['sparsity_l0'].mean(), color='blue', linestyle='--', \n               linewidth=2, label=f\"Mean: {training_logs['sparsity_l0'].mean():.4f}\")\n    ax.axvline(TARGET_SPARSITY_L0, color='red', linestyle='--', \n               linewidth=2, label=f'Target: {TARGET_SPARSITY_L0}')\n    ax.set_xlabel('Sparsity (L0)', fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)\n    ax.set_title('Sparsity Distribution', fontsize=14, pad=10)\n    ax.legend(fontsize=11)\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(ASSETS_DIR / f\"sae_sparsity_analysis_{STAGE}_{SAE_TYPE}_{LAYER_QUANTILE}.png\", \n                dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nSparsity Statistics:\")\n    print(f\"  Final sparsity: {training_logs['sparsity_l0'].iloc[-1]:.4f}\")\n    print(f\"  Mean sparsity:  {training_logs['sparsity_l0'].mean():.4f}\")\n    print(f\"  Target:         {TARGET_SPARSITY_L0:.4f}\")\n    print(f\"  Deviation:      {abs(training_logs['sparsity_l0'].iloc[-1] - TARGET_SPARSITY_L0):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "1. **Total Loss:** Overall training objective\n",
    "   - Should decrease and stabilize\n",
    "   - Final value indicates reconstruction quality\n",
    "\n",
    "2. **Reconstruction Loss:** How well SAE reconstructs inputs\n",
    "   - Lower is better\n",
    "   - Trade-off with sparsity\n",
    "\n",
    "3. **Sparsity Loss:** Penalty for non-sparse activations\n",
    "   - Controls feature selectivity\n",
    "   - Higher lambda = sparser features\n",
    "\n",
    "4. **Sparsity (L0):** Fraction of active features per input\n",
    "   - Target: ~5% (adjustable)\n",
    "   - Too low = information loss\n",
    "   - Too high = less interpretable\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "- **Convergence:** Loss should stabilize, not oscillate\n",
    "- **Balance:** Recon and sparsity losses should be similar magnitude\n",
    "- **L0 Target:** Should be close to target sparsity\n",
    "- **No Divergence:** Loss shouldn't increase over training\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. If L0 too high: Increase sparsity lambda\n",
    "2. If recon loss too high: Decrease sparsity lambda\n",
    "3. If not converged: Train longer\n",
    "4. If oscillating: Reduce learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}