{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Training Loss Visualization\n",
    "\n",
    "This notebook visualizes training dynamics for the Gated Sparse Autoencoder (gSAE).\n",
    "\n",
    "**Purpose:**\n",
    "- Monitor training progress and convergence\n",
    "- Compare loss components (reconstruction, sparsity)\n",
    "- Verify target sparsity is achieved\n",
    "- Identify training issues or anomalies\n",
    "\n",
    "**Input Data:**\n",
    "- Training logs: CSV with step, total_loss, recon_loss, sparsity_loss, sparsity_l0\n",
    "\n",
    "**Output:**\n",
    "- Multi-panel loss curves\n",
    "- Sparsity evolution\n",
    "- Training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.visualization import (\n",
    "    setup_korean_font,\n",
    "    load_training_logs,\n",
    "    plot_training_loss\n",
    ")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "setup_korean_font()\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "ASSETS_DIR = PROJECT_ROOT / \"notebooks\" / \"visualizations\" / \"assets\"\n",
    "ASSETS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "STAGE = \"mock\"\n",
    "\n",
    "print(f\"Stage: {STAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs\n",
    "logs = load_training_logs(RESULTS_DIR, stage=STAGE)\n",
    "\n",
    "print(f\"Training logs shape: {logs.shape}\")\n",
    "print(f\"Total steps: {len(logs)}\")\n",
    "print(f\"\\nColumns: {list(logs.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(logs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_loss(\n",
    "    training_logs=logs,\n",
    "    save_path=ASSETS_DIR / f\"sae_training_loss_{STAGE}.png\",\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed curves\n",
    "window = min(50, len(logs) // 10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Total loss (original + smoothed)\n",
    "ax = axes[0]\n",
    "ax.plot(logs['step'], logs['total_loss'], alpha=0.3, color='blue', label='Original')\n",
    "ax.plot(logs['step'], logs['total_loss'].rolling(window=window, center=True).mean(),\n",
    "        linewidth=2, color='blue', label=f'Smoothed (window={window})')\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Total Loss', fontsize=12)\n",
    "ax.set_title('전체 손실 (평활화)\\nTotal Loss (Smoothed)', fontsize=14, pad=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Loss components\n",
    "ax = axes[1]\n",
    "if 'recon_loss' in logs.columns:\n",
    "    ax.plot(logs['step'], logs['recon_loss'].rolling(window=window, center=True).mean(),\n",
    "            linewidth=2, color='green', label='Reconstruction Loss')\n",
    "if 'sparsity_loss' in logs.columns:\n",
    "    ax.plot(logs['step'], logs['sparsity_loss'].rolling(window=window, center=True).mean(),\n",
    "            linewidth=2, color='red', label='Sparsity Loss')\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('손실 구성 요소\\nLoss Components', fontsize=14, pad=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS_DIR / f\"sae_training_detailed_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sparsity_l0' in logs.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Sparsity over time\n",
    "    ax = axes[0]\n",
    "    ax.plot(logs['step'], logs['sparsity_l0'], linewidth=2, color='purple')\n",
    "    ax.axhline(0.05, color='red', linestyle='--', label='Target (5%)', alpha=0.7)\n",
    "    ax.set_xlabel('Step', fontsize=12)\n",
    "    ax.set_ylabel('Sparsity (L0)', fontsize=12)\n",
    "    ax.set_title('희소성 진화\\nSparsity Evolution', fontsize=14, pad=10)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Distribution of sparsity\n",
    "    ax = axes[1]\n",
    "    ax.hist(logs['sparsity_l0'], bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(logs['sparsity_l0'].mean(), color='blue', linestyle='--', \n",
    "               linewidth=2, label=f\"Mean: {logs['sparsity_l0'].mean():.3f}\")\n",
    "    ax.axvline(0.05, color='red', linestyle='--', linewidth=2, label='Target: 0.05')\n",
    "    ax.set_xlabel('Sparsity (L0)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title('희소성 분포\\nSparsity Distribution', fontsize=14, pad=10)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"sae_sparsity_analysis_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSparsity Statistics:\")\n",
    "    print(f\"  Final sparsity: {logs['sparsity_l0'].iloc[-1]:.4f}\")\n",
    "    print(f\"  Mean sparsity:  {logs['sparsity_l0'].mean():.4f}\")\n",
    "    print(f\"  Target:         0.0500\")\n",
    "    print(f\"  Deviation:      {abs(logs['sparsity_l0'].iloc[-1] - 0.05):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "stats = {}\n",
    "\n",
    "for col in ['total_loss', 'recon_loss', 'sparsity_loss', 'sparsity_l0']:\n",
    "    if col in logs.columns:\n",
    "        stats[col] = {\n",
    "            'Initial': logs[col].iloc[0],\n",
    "            'Final': logs[col].iloc[-1],\n",
    "            'Min': logs[col].min(),\n",
    "            'Max': logs[col].max(),\n",
    "            'Mean': logs[col].mean(),\n",
    "            'Std': logs[col].std(),\n",
    "            'Change': logs[col].iloc[-1] - logs[col].iloc[0],\n",
    "            'Change (%)': (logs[col].iloc[-1] - logs[col].iloc[0]) / logs[col].iloc[0] * 100\n",
    "        }\n",
    "\n",
    "df_stats = pd.DataFrame(stats).T\n",
    "\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(\"=\" * 100)\n",
    "print(df_stats.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "df_stats.to_csv(ASSETS_DIR / f\"training_statistics_{STAGE}.csv\")\n",
    "print(f\"\\nSaved to {ASSETS_DIR / f'training_statistics_{STAGE}.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence based on recent stability\n",
    "window_size = min(100, len(logs) // 5)\n",
    "recent_window = logs.tail(window_size)\n",
    "\n",
    "print(f\"\\nConvergence Analysis (last {window_size} steps):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in ['total_loss', 'recon_loss', 'sparsity_loss']:\n",
    "    if col in logs.columns:\n",
    "        mean = recent_window[col].mean()\n",
    "        std = recent_window[col].std()\n",
    "        cv = (std / mean) * 100  # Coefficient of variation\n",
    "        \n",
    "        converged = cv < 5.0  # <5% variation = converged\n",
    "        status = \"✓ Converged\" if converged else \"⚠ Not converged\"\n",
    "        \n",
    "        print(f\"{col:20s}: Mean={mean:.4f}, Std={std:.4f}, CV={cv:.2f}% {status}\")\n",
    "\n",
    "# Sparsity convergence\n",
    "if 'sparsity_l0' in logs.columns:\n",
    "    final_sparsity = logs['sparsity_l0'].iloc[-1]\n",
    "    target_sparsity = 0.05\n",
    "    deviation = abs(final_sparsity - target_sparsity)\n",
    "    \n",
    "    within_target = deviation < 0.01  # Within 1% of target\n",
    "    status = \"✓ Target achieved\" if within_target else \"⚠ Target not achieved\"\n",
    "    \n",
    "    print(f\"\\nSparsity Target:\")\n",
    "    print(f\"  Final:  {final_sparsity:.4f}\")\n",
    "    print(f\"  Target: {target_sparsity:.4f}\")\n",
    "    print(f\"  Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss gradient (rate of change)\n",
    "if len(logs) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    \n",
    "    # Compute gradient\n",
    "    gradient = np.gradient(logs['total_loss'].values)\n",
    "    smoothed_gradient = pd.Series(gradient).rolling(window=window, center=True).mean()\n",
    "    \n",
    "    ax.plot(logs['step'], smoothed_gradient, linewidth=2, color='orange')\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.set_xlabel('Step', fontsize=12)\n",
    "    ax.set_ylabel('Loss Gradient (rate of change)', fontsize=12)\n",
    "    ax.set_title('손실 변화율\\nLoss Rate of Change', fontsize=14, pad=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"loss_gradient_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nLoss Gradient Statistics:\")\n",
    "    print(f\"  Mean:   {np.mean(gradient):.6f}\")\n",
    "    print(f\"  Median: {np.median(gradient):.6f}\")\n",
    "    print(f\"  Std:    {np.std(gradient):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Total Loss:**\n",
    "   - Should decrease monotonically\n",
    "   - Converge to stable minimum\n",
    "   - No sudden spikes or divergence\n",
    "\n",
    "2. **Reconstruction Loss:**\n",
    "   - Measures how well SAE reconstructs inputs\n",
    "   - Should stabilize at low value\n",
    "   - Balance with sparsity constraint\n",
    "\n",
    "3. **Sparsity Loss:**\n",
    "   - Encourages sparse activations\n",
    "   - Should decrease as model learns sparsity\n",
    "   - Target: <5% active features (L0 norm)\n",
    "\n",
    "4. **Sparsity (L0):**\n",
    "   - Should converge to ~0.05 (5% active)\n",
    "   - Too high: Features not sparse enough\n",
    "   - Too low: May lose reconstruction quality\n",
    "\n",
    "### Training Quality Indicators:\n",
    "\n",
    "- ✓ **Good:** Smooth decrease, stable convergence, sparsity on target\n",
    "- ⚠ **Warning:** High variance, slow convergence, sparsity off target\n",
    "- ✗ **Bad:** Loss increase, divergence, training instability\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. If not converged: Continue training or adjust hyperparameters\n",
    "2. If sparsity off: Adjust sparsity coefficient\n",
    "3. If reconstruction poor: Reduce sparsity constraint\n",
    "4. If training good: Proceed to IG² computation and bias analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
