{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bias Feature Verification Analysis\n",
    "\n",
    "This notebook provides comprehensive visualizations comparing verification results across:\n",
    "- **3 EXAONE layers**: Q1 (25%), Q2 (50%), Q3 (75%)\n",
    "- **9 demographic dimensions**: 성별, 인종, 종교, 나이, 직업, 학력, 지역, 정치성향, 성적지향\n",
    "\n",
    "## Verification Tests:\n",
    "- **Suppression Test**: Setting identified bias features to 0 should reduce bias gap\n",
    "- **Amplification Test**: Multiplying bias features by 2 should increase bias gap  \n",
    "- **Random Control**: Suppressing random features should have minimal effect\n",
    "\n",
    "## Validation Criteria:\n",
    "1. Suppression reduces bias gap (negative change ratio)\n",
    "2. Amplification increases bias gap (positive change ratio)\n",
    "3. Effect is statistically significant vs random (|z-score| > 2)\n",
    "\n",
    "## Data Source:\n",
    "Results from `scripts/06_verify_bias_features.py` stored in:\n",
    "- `results/{stage}/{demographic}/verification/{layer_quantile}/suppression_test.json`\n",
    "- `results/{stage}/{demographic}/verification/{layer_quantile}/amplification_test.json`\n",
    "- `results/{stage}/{demographic}/verification/{layer_quantile}/random_control.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.visualization import ensure_korean_font\n",
    "from src.utils import load_json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Korean font\n",
    "font_name = ensure_korean_font()\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ASSETS_DIR = PROJECT_ROOT / \"notebooks\" / \"visualizations\" / \"assets\"\n",
    "ASSETS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Stage\n",
    "STAGE = \"full\"\n",
    "\n",
    "# Layer quantiles to compare\n",
    "LAYER_QUANTILES = [\"q1\", \"q2\", \"q3\"]\n",
    "LAYER_LABELS = {\n",
    "    \"q1\": \"Layer Q1 (25%)\",\n",
    "    \"q2\": \"Layer Q2 (50%)\",\n",
    "    \"q3\": \"Layer Q3 (75%)\"\n",
    "}\n",
    "\n",
    "# Load demographics\n",
    "demo_dict = load_json(DATA_DIR / \"demographic_dict_ko.json\")\n",
    "DEMOGRAPHICS = list(demo_dict.keys())\n",
    "DEMOGRAPHIC_EN = {d: demo_dict[d]['dimension_en'] for d in DEMOGRAPHICS}\n",
    "\n",
    "print(f\"Stage: {STAGE}\")\n",
    "print(f\"Layer Quantiles: {LAYER_QUANTILES}\")\n",
    "print(f\"\\nDemographics ({len(DEMOGRAPHICS)}):\")\n",
    "for d in DEMOGRAPHICS:\n",
    "    print(f\"  - {d} ({DEMOGRAPHIC_EN[d]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_verification_data(results_dir, stage, demographic, layer_quantile):\n",
    "    \"\"\"\n",
    "    Load verification results for a specific demographic and layer.\n",
    "    \n",
    "    Supports both:\n",
    "    - New structure: results/{stage}/{demographic}/verification/{layer_quantile}/\n",
    "    - Legacy structure: results/{stage}/{demographic}/verification/\n",
    "    \n",
    "    Returns:\n",
    "        dict with suppression, amplification, and random control data\n",
    "        or None if not found\n",
    "    \"\"\"\n",
    "    # Try new layer-specific structure first\n",
    "    verif_dir = results_dir / stage / demographic / 'verification' / layer_quantile\n",
    "    \n",
    "    # Fallback to legacy structure (no layer subdirectory)\n",
    "    if not verif_dir.exists():\n",
    "        verif_dir = results_dir / stage / demographic / 'verification'\n",
    "        if not verif_dir.exists():\n",
    "            return None\n",
    "    \n",
    "    result = {'layer_quantile': layer_quantile}\n",
    "    \n",
    "    # Load suppression test\n",
    "    suppress_path = verif_dir / 'suppression_test.json'\n",
    "    if suppress_path.exists():\n",
    "        result['suppression'] = load_json(suppress_path)\n",
    "    \n",
    "    # Load amplification test\n",
    "    amplify_path = verif_dir / 'amplification_test.json'\n",
    "    if amplify_path.exists():\n",
    "        result['amplification'] = load_json(amplify_path)\n",
    "    \n",
    "    # Load random control\n",
    "    random_path = verif_dir / 'random_control.json'\n",
    "    if random_path.exists():\n",
    "        result['random'] = load_json(random_path)\n",
    "    \n",
    "    return result if len(result) > 1 else None\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all verification results for all demographics and layers\n",
    "verification_data = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "print(\"Scanning for available verification results...\\n\")\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        data = load_verification_data(RESULTS_DIR, STAGE, demo, lq)\n",
    "        if data:\n",
    "            verification_data[demo][lq] = data\n",
    "\n",
    "# Summary table\n",
    "print(\"Available Verification Results Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Demographic':<15} | {'Q1 (25%)':<15} | {'Q2 (50%)':<15} | {'Q3 (75%)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    row = f\"{demo:<15} |\"\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        if demo in verification_data and lq in verification_data[demo]:\n",
    "            has_supp = 'suppression' in verification_data[demo][lq]\n",
    "            has_amp = 'amplification' in verification_data[demo][lq]\n",
    "            has_rand = 'random' in verification_data[demo][lq]\n",
    "            status = f\"S:{'+' if has_supp else '-'} A:{'+' if has_amp else '-'} R:{'+' if has_rand else '-'}\"\n",
    "        else:\n",
    "            status = \"--\"\n",
    "        row += f\" {status:<14}|\"\n",
    "    print(row)\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "for demo in DEMOGRAPHICS:\n",
    "    for lq in LAYER_QUANTILES:\n",
    "        if demo not in verification_data or lq not in verification_data[demo]:\n",
    "            continue\n",
    "        \n",
    "        data = verification_data[demo][lq]\n",
    "        \n",
    "        row = {\n",
    "            'Demographic': demo,\n",
    "            'Demographic_EN': DEMOGRAPHIC_EN[demo],\n",
    "            'Layer': LAYER_LABELS[lq],\n",
    "            'Layer_Quantile': lq,\n",
    "        }\n",
    "        \n",
    "        # Suppression data\n",
    "        if 'suppression' in data:\n",
    "            supp = data['suppression']\n",
    "            row['Suppress_Gap_Before'] = supp.get('gap_before', 0)\n",
    "            row['Suppress_Gap_After'] = supp.get('gap_after', 0)\n",
    "            row['Suppress_Change_Ratio'] = supp.get('gap_change_ratio', 0)\n",
    "            row['Suppress_Num_Features'] = supp.get('metadata', {}).get('num_features_manipulated', 0)\n",
    "            row['Suppress_Gap_Std_Before'] = supp.get('metadata', {}).get('gap_std_before', 0)\n",
    "            row['Suppress_Gap_Std_After'] = supp.get('metadata', {}).get('gap_std_after', 0)\n",
    "        \n",
    "        # Amplification data\n",
    "        if 'amplification' in data:\n",
    "            amp = data['amplification']\n",
    "            row['Amplify_Gap_Before'] = amp.get('gap_before', 0)\n",
    "            row['Amplify_Gap_After'] = amp.get('gap_after', 0)\n",
    "            row['Amplify_Change_Ratio'] = amp.get('gap_change_ratio', 0)\n",
    "        \n",
    "        # Random control data\n",
    "        if 'random' in data:\n",
    "            rand = data['random']\n",
    "            row['Random_Mean_Change'] = rand.get('mean_gap_change', 0)\n",
    "            row['Random_Std_Change'] = rand.get('std_gap_change', 0)\n",
    "            row['Random_Num_Trials'] = rand.get('num_trials', 0)\n",
    "            \n",
    "            # Calculate Z-score for suppression effect\n",
    "            if row.get('Suppress_Change_Ratio') is not None and rand.get('std_gap_change', 0) > 0:\n",
    "                z_score = (row['Suppress_Change_Ratio'] - rand['mean_gap_change']) / rand['std_gap_change']\n",
    "                row['Z_Score'] = z_score\n",
    "            else:\n",
    "                row['Z_Score'] = 0\n",
    "        \n",
    "        summary_data.append(row)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "if len(df_summary) > 0:\n",
    "    print(f\"\\nCollected verification data for {len(df_summary)} experiments\")\n",
    "    display_cols = ['Demographic_EN', 'Layer_Quantile', 'Suppress_Change_Ratio', \n",
    "                    'Amplify_Change_Ratio', 'Z_Score', 'Suppress_Num_Features']\n",
    "    print(df_summary[display_cols].head(15).to_string(index=False))\n",
    "else:\n",
    "    print(\"No verification data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Suppression Effect Heatmap by Layer and Demographic\n",
    "\n",
    "### How It's Computed:\n",
    "Suppression sets identified bias features to zero and measures the change in logit gap (difference between correct and incorrect demographic predictions). The change ratio = (gap_after - gap_before) / gap_before. Negative values indicate bias reduction - the model becomes less confident in distinguishing demographics.\n",
    "\n",
    "### Analysis:\n",
    "Green cells (negative values) indicate successful bias reduction through feature suppression, confirming these features causally encode bias. Darker green means stronger effect. If suppression doesn't reduce bias (red/neutral), the identified features may not be causally related to bias predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0:\n",
    "    # Create ordered list of English labels matching DEMOGRAPHICS order\n",
    "    demo_en_ordered = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "    \n",
    "    # Pivot for suppression change ratio\n",
    "    pivot_suppress = df_summary.pivot(index='Demographic_EN', columns='Layer_Quantile', values='Suppress_Change_Ratio')\n",
    "    pivot_suppress = pivot_suppress.reindex(demo_en_ordered)  # Reorder rows\n",
    "    pivot_suppress = pivot_suppress[['q1', 'q2', 'q3']] * 100  # Convert to percentage\n",
    "    pivot_suppress.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Use diverging colormap: GREEN (negative/good) - white (0) - RED (positive/bad)\n",
    "    # RdYlGn_r reversed so green=negative (bias reduction = good)\n",
    "    sns.heatmap(\n",
    "        pivot_suppress,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlGn_r',  # Reversed: Green for negative (good), Red for positive (bad)\n",
    "        center=0,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Gap Change (%)'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 11}\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Suppression Effect: Gap Change Ratio (%)\\nby Layer and Demographic\\n(Green/Negative = Bias Reduced = Good)', \n",
    "                 fontsize=14, pad=15)\n",
    "    ax.set_xlabel('EXAONE Layer Quantile', fontsize=12)\n",
    "    ax.set_ylabel('Demographic Dimension', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_suppress_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nSuppression Effect Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    for lq in ['q1', 'q2', 'q3']:\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        if len(lq_data) > 0:\n",
    "            print(f\"\\n{LAYER_LABELS[lq]}:\")\n",
    "            print(f\"  Mean Change:  {lq_data['Suppress_Change_Ratio'].mean()*100:.2f}%\")\n",
    "            print(f\"  Std Change:   {lq_data['Suppress_Change_Ratio'].std()*100:.2f}%\")\n",
    "            print(f\"  Min Change:   {lq_data['Suppress_Change_Ratio'].min()*100:.2f}%\")\n",
    "            print(f\"  Max Change:   {lq_data['Suppress_Change_Ratio'].max()*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No verification data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Amplification Effect Heatmap by Layer and Demographic\n",
    "\n",
    "### How It's Computed:\n",
    "Amplification multiplies identified bias features by 2 (doubling their activation magnitude) and measures the change in logit gap. The change ratio = (gap_after - gap_before) / gap_before. Positive values indicate bias amplification - the model becomes more confident in distinguishing demographics.\n",
    "\n",
    "### Analysis:\n",
    "Red cells (positive values) confirm that the identified features causally increase bias when amplified. This is the expected behavior for true bias features. If amplification doesn't increase bias, the features may not be directly responsible for biased predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0 and 'Amplify_Change_Ratio' in df_summary.columns:\n",
    "    # Pivot for amplification change ratio\n",
    "    pivot_amplify = df_summary.pivot(index='Demographic_EN', columns='Layer_Quantile', values='Amplify_Change_Ratio')\n",
    "    pivot_amplify = pivot_amplify.reindex(demo_en_ordered)  # Reorder rows\n",
    "    pivot_amplify = pivot_amplify[['q1', 'q2', 'q3']] * 100  # Convert to percentage\n",
    "    pivot_amplify.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Use diverging colormap: blue (negative/unexpected) - white (0) - red (positive/expected)\n",
    "    sns.heatmap(\n",
    "        pivot_amplify,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdBu_r',  # Red for positive (expected), Blue for negative (unexpected)\n",
    "        center=0,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Gap Change (%)'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 11}\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Amplification Effect: Gap Change Ratio (%)\\nby Layer and Demographic\\n(Red/Positive = Bias Increased = Expected)', \n",
    "                 fontsize=14, pad=15)\n",
    "    ax.set_xlabel('EXAONE Layer Quantile', fontsize=12)\n",
    "    ax.set_ylabel('Demographic Dimension', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_amplify_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nAmplification Effect Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    for lq in ['q1', 'q2', 'q3']:\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        if len(lq_data) > 0:\n",
    "            print(f\"\\n{LAYER_LABELS[lq]}:\")\n",
    "            print(f\"  Mean Change:  {lq_data['Amplify_Change_Ratio'].mean()*100:.2f}%\")\n",
    "            print(f\"  Std Change:   {lq_data['Amplify_Change_Ratio'].std()*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No amplification data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Z-Score Heatmap (Statistical Significance)\n",
    "\n",
    "### How It's Computed:\n",
    "Z-score compares the suppression effect to random control: Z = (suppress_change - random_mean) / random_std. It measures how many standard deviations the bias feature suppression differs from suppressing random features. |Z| > 2 indicates statistical significance (p < 0.05).\n",
    "\n",
    "### Analysis:\n",
    "Cells with |Z| > 2 (dark colors) indicate the identified bias features have a significantly different effect than random features. This confirms the features are specifically related to bias, not just general model behavior. Non-significant results suggest the identified features may not be truly bias-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0 and 'Z_Score' in df_summary.columns:\n",
    "    # Pivot for Z-scores\n",
    "    pivot_zscore = df_summary.pivot(index='Demographic_EN', columns='Layer_Quantile', values='Z_Score')\n",
    "    pivot_zscore = pivot_zscore.reindex(demo_en_ordered)  # Reorder rows\n",
    "    pivot_zscore = pivot_zscore[['q1', 'q2', 'q3']]\n",
    "    pivot_zscore.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Custom colormap for significance (green for negative z-scores = good)\n",
    "    sns.heatmap(\n",
    "        pivot_zscore,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlGn_r',  # Green for negative (significant reduction)\n",
    "        center=0,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Z-Score'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 11}\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Statistical Significance: Z-Score\\nby Layer and Demographic\\n(|Z| > 2 = Significant, Green = Bias Reduction)', \n",
    "                 fontsize=14, pad=15)\n",
    "    ax.set_xlabel('EXAONE Layer Quantile', fontsize=12)\n",
    "    ax.set_ylabel('Demographic Dimension', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_zscore_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary of significance\n",
    "    print(\"\\nStatistical Significance Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    for lq in ['q1', 'q2', 'q3']:\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        if len(lq_data) > 0:\n",
    "            significant = len(lq_data[abs(lq_data['Z_Score']) > 2])\n",
    "            print(f\"{LAYER_LABELS[lq]}: {significant}/{len(lq_data)} demographics significant (|z| > 2)\")\n",
    "else:\n",
    "    print(\"No Z-score data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Grouped Bar Chart: Comparison by Layer\n",
    "\n",
    "### How It's Computed:\n",
    "This visualization directly compares suppression and amplification effects across all layers for each demographic. Each bar represents the gap change ratio (%) for a specific layer. The bars are grouped by demographic dimension.\n",
    "\n",
    "### Analysis:\n",
    "Consistent negative bars for suppression and positive bars for amplification across layers confirms robust causal relationships. Layer-specific differences reveal where in the model bias features are most effective. Inconsistent results across layers may indicate layer-specific bias encoding patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Get English labels in order\n",
    "    demographics_en = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "    \n",
    "    x = np.arange(len(DEMOGRAPHICS))\n",
    "    width = 0.25\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']  # Blue, Green, Red for Q1, Q2, Q3\n",
    "    \n",
    "    # === Left: Suppression Effect ===\n",
    "    ax = axes[0]\n",
    "    \n",
    "    for i, lq in enumerate(LAYER_QUANTILES):\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        \n",
    "        # Get suppression change for each demographic (in order)\n",
    "        changes = []\n",
    "        for demo in DEMOGRAPHICS:\n",
    "            demo_data = lq_data[lq_data['Demographic'] == demo]\n",
    "            if len(demo_data) > 0:\n",
    "                changes.append(demo_data['Suppress_Change_Ratio'].values[0] * 100)\n",
    "            else:\n",
    "                changes.append(0)\n",
    "        \n",
    "        ax.bar(x + i * width, changes, width, label=LAYER_LABELS[lq], color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "    ax.set_xlabel('Demographic Dimension', fontsize=12)\n",
    "    ax.set_ylabel('Gap Change (%)', fontsize=12)\n",
    "    ax.set_title('Suppression Effect by Layer\\n(Negative = Bias Reduced = Good)', fontsize=14, pad=15)\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(demographics_en, rotation=45, ha='right', fontsize=10)\n",
    "    ax.legend(title='Layer', loc='upper right')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # === Right: Amplification Effect ===\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    for i, lq in enumerate(LAYER_QUANTILES):\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        \n",
    "        # Get amplification change for each demographic (in order)\n",
    "        changes = []\n",
    "        for demo in DEMOGRAPHICS:\n",
    "            demo_data = lq_data[lq_data['Demographic'] == demo]\n",
    "            if len(demo_data) > 0 and 'Amplify_Change_Ratio' in demo_data.columns:\n",
    "                changes.append(demo_data['Amplify_Change_Ratio'].values[0] * 100)\n",
    "            else:\n",
    "                changes.append(0)\n",
    "        \n",
    "        ax2.bar(x + i * width, changes, width, label=LAYER_LABELS[lq], color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax2.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "    ax2.set_xlabel('Demographic Dimension', fontsize=12)\n",
    "    ax2.set_ylabel('Gap Change (%)', fontsize=12)\n",
    "    ax2.set_title('Amplification Effect by Layer\\n(Positive = Bias Increased = Expected)', fontsize=14, pad=15)\n",
    "    ax2.set_xticks(x + width)\n",
    "    ax2.set_xticklabels(demographics_en, rotation=45, ha='right', fontsize=10)\n",
    "    ax2.legend(title='Layer', loc='upper right')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_layer_comparison_bars_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No verification data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Per-Demographic Verification Effects (Detailed View)\n",
    "\n",
    "### How It's Computed:\n",
    "For each demographic, this shows the absolute logit gap values: baseline (original), after suppression, and after amplification. The logit gap measures the model's confidence in distinguishing demographic groups - higher values indicate stronger bias encoding.\n",
    "\n",
    "### Analysis:\n",
    "Successful verification shows: suppression bar lower than baseline (reduced bias) and amplification bar higher than baseline (increased bias). Comparing across layers reveals which layer's bias features are most effective for each demographic. Consistent patterns across demographics indicate generalizable bias encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0:\n",
    "    n_demographics = len(DEMOGRAPHICS)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_demographics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    fig.suptitle(\n",
    "        'Verification Effects by Demographic and Layer\\n'\n",
    "        '(Baseline / Suppression / Amplification)',\n",
    "        fontsize=18,\n",
    "        y=0.995\n",
    "    )\n",
    "    \n",
    "    colors = {'q1': '#3498db', 'q2': '#2ecc71', 'q3': '#e74c3c'}\n",
    "    \n",
    "    for idx, demo in enumerate(DEMOGRAPHICS):\n",
    "        ax = axes[idx]\n",
    "        demo_en = DEMOGRAPHIC_EN.get(demo, demo)\n",
    "        \n",
    "        # Get data for all layers for this demographic\n",
    "        demo_data = df_summary[df_summary['Demographic'] == demo].sort_values('Layer_Quantile')\n",
    "        \n",
    "        if len(demo_data) == 0:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "            ax.set_title(f\"{demo}\\n({demo_en})\", fontsize=13)\n",
    "            continue\n",
    "        \n",
    "        x_labels = ['Baseline', 'Suppress', 'Amplify']\n",
    "        x_pos = np.arange(len(x_labels))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (_, row) in enumerate(demo_data.iterrows()):\n",
    "            lq = row['Layer_Quantile']\n",
    "            baseline = row.get('Suppress_Gap_Before', 0)\n",
    "            suppress = row.get('Suppress_Gap_After', 0)\n",
    "            amplify = row.get('Amplify_Gap_After', 0)\n",
    "            \n",
    "            values = [baseline, suppress, amplify]\n",
    "            ax.bar(x_pos + i * width, values, width, label=LAYER_LABELS[lq], \n",
    "                   color=colors[lq], alpha=0.8)\n",
    "        \n",
    "        ax.set_xticks(x_pos + width)\n",
    "        ax.set_xticklabels(x_labels, fontsize=11)\n",
    "        ax.set_ylabel('Logit Gap', fontsize=11)\n",
    "        ax.set_title(f\"{demo}\\n({demo_en})\", fontsize=13, pad=10)\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for i in range(n_demographics, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_by_demographic_detail_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No verification data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Validation Criteria Summary\n",
    "\n",
    "### How It's Computed:\n",
    "Three validation criteria are checked: (C1) suppression change ratio < 0, (C2) amplification change ratio > 0, (C3) |Z-score| > 2. Each criterion is binary pass/fail. Total criteria met counts how many of the 3 criteria pass for each demographic-layer combination.\n",
    "\n",
    "### Analysis:\n",
    "Green cells (pass) indicate the identified bias features satisfy the validation criterion. All 3 criteria passing confirms robust causal bias encoding. Partial passes suggest the features may have weaker or more complex relationships with bias. Demographics/layers with 0 passes need re-examination of feature identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0:\n",
    "    # Compute validation criteria\n",
    "    df_validation = df_summary.copy()\n",
    "    \n",
    "    # Criterion 1: Suppression reduces bias (change ratio < 0)\n",
    "    df_validation['C1_Suppress_Reduces'] = df_validation['Suppress_Change_Ratio'] < 0\n",
    "    \n",
    "    # Criterion 2: Amplification increases bias (change ratio > 0)\n",
    "    df_validation['C2_Amplify_Increases'] = df_validation['Amplify_Change_Ratio'] > 0\n",
    "    \n",
    "    # Criterion 3: Statistically significant (|z| > 2)\n",
    "    df_validation['C3_Significant'] = abs(df_validation['Z_Score']) > 2\n",
    "    \n",
    "    # Count criteria met\n",
    "    df_validation['Criteria_Met'] = (\n",
    "        df_validation['C1_Suppress_Reduces'].astype(int) +\n",
    "        df_validation['C2_Amplify_Increases'].astype(int) +\n",
    "        df_validation['C3_Significant'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Create validation heatmaps for each criterion\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    \n",
    "    demo_en_ordered = [DEMOGRAPHIC_EN[d] for d in DEMOGRAPHICS]\n",
    "    \n",
    "    criteria_info = [\n",
    "        ('C1_Suppress_Reduces', 'C1: Suppression Reduces Bias', 'RdYlGn'),\n",
    "        ('C2_Amplify_Increases', 'C2: Amplification Increases Bias', 'RdYlGn'),\n",
    "        ('C3_Significant', 'C3: Statistically Significant (|z|>2)', 'RdYlGn'),\n",
    "        ('Criteria_Met', 'Total Criteria Met (0-3)', 'RdYlGn'),\n",
    "    ]\n",
    "    \n",
    "    for idx, (col, title, cmap) in enumerate(criteria_info):\n",
    "        ax = axes.flatten()[idx]\n",
    "        \n",
    "        pivot = df_validation.pivot(index='Demographic_EN', columns='Layer_Quantile', values=col)\n",
    "        pivot = pivot.reindex(demo_en_ordered)\n",
    "        pivot = pivot[['q1', 'q2', 'q3']]\n",
    "        pivot.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "        \n",
    "        if col == 'Criteria_Met':\n",
    "            vmin, vmax = 0, 3\n",
    "            fmt = 'd'\n",
    "            pivot = pivot.astype(int)\n",
    "        else:\n",
    "            vmin, vmax = 0, 1\n",
    "            fmt = 'd'\n",
    "            pivot = pivot.astype(int)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            annot=True,\n",
    "            fmt=fmt,\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            linewidths=0.5,\n",
    "            annot_kws={'fontsize': 12, 'fontweight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        ax.set_title(title, fontsize=13, pad=10)\n",
    "        ax.set_xlabel('Layer', fontsize=11)\n",
    "        ax.set_ylabel('Demographic', fontsize=11)\n",
    "    \n",
    "    plt.suptitle('Validation Criteria by Layer and Demographic\\n(1=Pass, 0=Fail)', fontsize=16, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_validation_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"VALIDATION SUMMARY BY LAYER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for lq in LAYER_QUANTILES:\n",
    "        lq_data = df_validation[df_validation['Layer_Quantile'] == lq]\n",
    "        if len(lq_data) > 0:\n",
    "            c1_pass = lq_data['C1_Suppress_Reduces'].sum()\n",
    "            c2_pass = lq_data['C2_Amplify_Increases'].sum()\n",
    "            c3_pass = lq_data['C3_Significant'].sum()\n",
    "            all_pass = len(lq_data[lq_data['Criteria_Met'] == 3])\n",
    "            \n",
    "            print(f\"\\n{LAYER_LABELS[lq]}:\")\n",
    "            print(f\"  C1 (Suppress reduces):    {c1_pass}/{len(lq_data)} pass\")\n",
    "            print(f\"  C2 (Amplify increases):   {c2_pass}/{len(lq_data)} pass\")\n",
    "            print(f\"  C3 (Significant |z|>2):   {c3_pass}/{len(lq_data)} pass\")\n",
    "            print(f\"  All 3 criteria:           {all_pass}/{len(lq_data)} pass\")\n",
    "else:\n",
    "    print(\"No data available for validation summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Number of Bias Features by Layer and Demographic\n",
    "\n",
    "### How It's Computed:\n",
    "This counts how many SAE features were identified as \"bias features\" through IG² thresholding for each demographic-layer combination. These are the features that were manipulated in suppression and amplification tests.\n",
    "\n",
    "### Analysis:\n",
    "Fewer features (lighter blue) indicate concentrated bias encoding - easier to intervene on. More features (darker blue) suggest diffuse bias encoding. Layer variations show where bias is most/least concentrated. Demographics with very few features may have insufficient signal for reliable verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0 and 'Suppress_Num_Features' in df_summary.columns:\n",
    "    # Pivot for number of features\n",
    "    pivot_features = df_summary.pivot(index='Demographic_EN', columns='Layer_Quantile', values='Suppress_Num_Features')\n",
    "    pivot_features = pivot_features.reindex(demo_en_ordered)  # Reorder rows\n",
    "    pivot_features = pivot_features[['q1', 'q2', 'q3']].astype(int)\n",
    "    pivot_features.columns = ['Q1 (25%)', 'Q2 (50%)', 'Q3 (75%)']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_features,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Number of Features'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'fontsize': 11}\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Number of Identified Bias Features\\nby Layer and Demographic', \n",
    "                 fontsize=14, pad=15)\n",
    "    ax.set_xlabel('EXAONE Layer Quantile', fontsize=12)\n",
    "    ax.set_ylabel('Demographic Dimension', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f\"verification_num_features_heatmap_{STAGE}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nNumber of Bias Features Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    for lq in ['q1', 'q2', 'q3']:\n",
    "        lq_data = df_summary[df_summary['Layer_Quantile'] == lq]\n",
    "        if len(lq_data) > 0:\n",
    "            print(f\"\\n{LAYER_LABELS[lq]}:\")\n",
    "            print(f\"  Mean:  {lq_data['Suppress_Num_Features'].mean():.1f}\")\n",
    "            print(f\"  Min:   {lq_data['Suppress_Num_Features'].min():.0f}\")\n",
    "            print(f\"  Max:   {lq_data['Suppress_Num_Features'].max():.0f}\")\n",
    "else:\n",
    "    print(\"No feature count data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Comprehensive Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_summary) > 0:\n",
    "    # Create final summary table\n",
    "    export_cols = [\n",
    "        'Demographic', 'Demographic_EN', 'Layer_Quantile',\n",
    "        'Suppress_Gap_Before', 'Suppress_Gap_After', 'Suppress_Change_Ratio',\n",
    "        'Amplify_Gap_After', 'Amplify_Change_Ratio',\n",
    "        'Random_Mean_Change', 'Random_Std_Change', 'Z_Score',\n",
    "        'Suppress_Num_Features'\n",
    "    ]\n",
    "    \n",
    "    final_summary = df_summary[[col for col in export_cols if col in df_summary.columns]].copy()\n",
    "    \n",
    "    # Add validation columns\n",
    "    final_summary['C1_Suppress_Reduces'] = final_summary['Suppress_Change_Ratio'] < 0\n",
    "    final_summary['C2_Amplify_Increases'] = final_summary['Amplify_Change_Ratio'] > 0\n",
    "    final_summary['C3_Significant'] = abs(final_summary['Z_Score']) > 2\n",
    "    final_summary['All_Criteria_Pass'] = (\n",
    "        final_summary['C1_Suppress_Reduces'] & \n",
    "        final_summary['C2_Amplify_Increases'] & \n",
    "        final_summary['C3_Significant']\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = ASSETS_DIR / f\"verification_layer_demographic_summary_{STAGE}.csv\"\n",
    "    final_summary.to_csv(output_path, index=False)\n",
    "    print(f\"Summary saved to: {output_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"COMPREHENSIVE VERIFICATION SUMMARY\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    display_summary = final_summary[[\n",
    "        'Demographic_EN', 'Layer_Quantile',\n",
    "        'Suppress_Change_Ratio', 'Amplify_Change_Ratio',\n",
    "        'Z_Score', 'Suppress_Num_Features', 'All_Criteria_Pass'\n",
    "    ]].copy()\n",
    "    display_summary['Suppress_Change_Ratio'] = display_summary['Suppress_Change_Ratio'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    display_summary['Amplify_Change_Ratio'] = display_summary['Amplify_Change_Ratio'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    display_summary['Z_Score'] = display_summary['Z_Score'].apply(lambda x: f\"{x:.2f}\")\n",
    "    display_summary.columns = ['Demographic', 'Layer', 'Suppress', 'Amplify', 'Z-Score', 'Num Feat', 'All Pass']\n",
    "    print(display_summary.to_string(index=False))\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    total = len(final_summary)\n",
    "    all_pass = final_summary['All_Criteria_Pass'].sum()\n",
    "    print(f\"OVERALL: {all_pass}/{total} layer-demographic combinations pass all criteria ({all_pass/total*100:.1f}%)\")\n",
    "    print(\"=\" * 120)\n",
    "else:\n",
    "    print(\"No data available for export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Verification Visualization Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nAssets saved to: {ASSETS_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(ASSETS_DIR.glob(f\"verification*_{STAGE}*\")):\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
